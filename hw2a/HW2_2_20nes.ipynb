{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO TO Joyanthan Nanduri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def parse_metadata_and_body(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='latin1') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    meta = {}\n",
    "    lines = content.split(\"\\n\")\n",
    "    body_start = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \": \" in line:\n",
    "            key, value = line.split(\": \", 1)\n",
    "            meta[key.strip()] = value.strip()\n",
    "        else:\n",
    "            body_start = i + 1\n",
    "            break\n",
    "\n",
    "    body = \"\\n\".join(lines[body_start:]).strip()\n",
    "    return meta, body\n",
    "\n",
    "def create_csv_from_newsgroups(data_dir, output_csv):\n",
    "    \n",
    "    fieldnames = [\"id\",\"label\", \"from\", \"subject\",  \"lines\", \"organization\", \"body\"]\n",
    "    \n",
    "    with open(output_csv, mode='w', encoding='utf-8', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for category in os.listdir(data_dir):\n",
    "            category_dir = os.path.join(data_dir, category)\n",
    "            if not os.path.isdir(category_dir):\n",
    "                continue\n",
    "            \n",
    "            for file_name in os.listdir(category_dir):\n",
    "                file_path = os.path.join(category_dir, file_name)\n",
    "                metadata, body = parse_metadata_and_body(file_path)\n",
    "                \n",
    "                writer.writerow({\n",
    "                    \"id\": file_name,\n",
    "                    \"label\": category,\n",
    "                    \"from\": metadata.get(\"From\", \"\"),\n",
    "                    \"subject\": metadata.get(\"Subject\", \"\"),\n",
    "                    \"lines\": metadata.get(\"Lines\", \"\"),\n",
    "                    \"organization\": metadata.get(\"Organization\", \"\"),\n",
    "                    \"body\": body\n",
    "                })\n",
    "\n",
    "train_data_dir = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news-bydate/20news-bydate-train\"\n",
    "train_output_csv = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news_train.csv\"\n",
    "\n",
    "create_csv_from_newsgroups(train_data_dir, train_output_csv)\n",
    "\n",
    "test_data_dir = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news-bydate/20news-bydate-test\"\n",
    "test_output_csv = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news_test.csv\"\n",
    "\n",
    "create_csv_from_newsgroups(test_data_dir, test_output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data number : 18846\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_csv = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news_train.csv\"\n",
    "test_csv = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news_test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"data number : {len(combined_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>from</th>\n",
       "      <th>subject</th>\n",
       "      <th>lines</th>\n",
       "      <th>organization</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76198</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>Peter Todd Chan &lt;pc1o+@andrew.cmu.edu&gt;</td>\n",
       "      <td>*REDUCED* Sony CD Players 4 Sale</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Fifth yr. senior, Electrical and Computer Engi...</td>\n",
       "      <td>ITEM: Sony ES-CDPX229\\nCONDITION: mint\\nAGE: 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76341</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>warren@nysernet.org (Warren Burstein)</td>\n",
       "      <td>Re: To be exact, 2.5 million Muslims were exte...</td>\n",
       "      <td>34</td>\n",
       "      <td>NYSERNet, Inc.</td>\n",
       "      <td>ac = In &lt;9304202017@zuma.UUCP&gt; sera@zuma.UUCP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60428</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>richk@grebyn.com (Richard Krehbiel)</td>\n",
       "      <td>Re: IDE vs SCSI</td>\n",
       "      <td>12</td>\n",
       "      <td>Grebyn Timesharing, Inc.</td>\n",
       "      <td>In article &lt;1qm5c9$6on@hcx1.ssd.csd.harris.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>179009</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>rodger-scoggin@ksc.nasa.gov (Rodger C. Scoggin)</td>\n",
       "      <td>Re: The earth also pollutes &amp; some scientists ...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In article &lt;C5uDn9.Gr@ncratl.AtlantaGA.NCR.COM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178545</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>smith@phoneme.harvard.edu (Steven Smith)</td>\n",
       "      <td>The Manitoban Candidate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harvard Robotics Lab, Harvard University</td>\n",
       "      <td>Lines: 18\\n\\nbross@sandbanks.cosc.brocku.ca (B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                     label  \\\n",
       "0   76198              misc.forsale   \n",
       "1   76341     talk.politics.mideast   \n",
       "2   60428  comp.sys.ibm.pc.hardware   \n",
       "3  179009        talk.politics.misc   \n",
       "4  178545        talk.politics.misc   \n",
       "\n",
       "                                              from  \\\n",
       "0           Peter Todd Chan <pc1o+@andrew.cmu.edu>   \n",
       "1            warren@nysernet.org (Warren Burstein)   \n",
       "2              richk@grebyn.com (Richard Krehbiel)   \n",
       "3  rodger-scoggin@ksc.nasa.gov (Rodger C. Scoggin)   \n",
       "4         smith@phoneme.harvard.edu (Steven Smith)   \n",
       "\n",
       "                                             subject lines  \\\n",
       "0                   *REDUCED* Sony CD Players 4 Sale  22.0   \n",
       "1  Re: To be exact, 2.5 million Muslims were exte...    34   \n",
       "2                                    Re: IDE vs SCSI    12   \n",
       "3  Re: The earth also pollutes & some scientists ...  32.0   \n",
       "4                            The Manitoban Candidate   NaN   \n",
       "\n",
       "                                        organization  \\\n",
       "0  Fifth yr. senior, Electrical and Computer Engi...   \n",
       "1                                     NYSERNet, Inc.   \n",
       "2                           Grebyn Timesharing, Inc.   \n",
       "3                                                NaN   \n",
       "4           Harvard Robotics Lab, Harvard University   \n",
       "\n",
       "                                                body  \n",
       "0  ITEM: Sony ES-CDPX229\\nCONDITION: mint\\nAGE: 1...  \n",
       "1  ac = In <9304202017@zuma.UUCP> sera@zuma.UUCP ...  \n",
       "2  In article <1qm5c9$6on@hcx1.ssd.csd.harris.com...  \n",
       "3  In article <C5uDn9.Gr@ncratl.AtlantaGA.NCR.COM...  \n",
       "4  Lines: 18\\n\\nbross@sandbanks.cosc.brocku.ca (B...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization, Tfidf vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_20 = combined_data.dropna(subset = ['body']).reset_index(drop = True)\n",
    "train_documents = data_20['body'].reset_index(drop=True)\n",
    "train_labels = data_20['label'].reset_index(drop=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000, \n",
    "    lowercase=True, \n",
    "    stop_words=\"english\", \n",
    "    max_df=0.8, \n",
    "    min_df=5\n",
    ")\n",
    "data_20_vecotorize = vectorizer.fit_transform(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18817, 10000)\n",
      "(18817,)\n"
     ]
    }
   ],
   "source": [
    "print(data_20_vecotorize.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "\n",
    "\n",
    "\n",
    "def compute_kmeans_objective(data, labels, centroids):\n",
    " \n",
    "    labels = labels.astype(int)\n",
    "    sq_distances = np.sum((data - centroids[labels]) ** 2, axis=1)\n",
    "    return np.sum(sq_distances)\n",
    "\n",
    "def compute_purity_gini(pred_labels, true_labels):\n",
    "\n",
    "    from collections import Counter\n",
    "    total = len(true_labels)\n",
    "    cluster_counts = {}\n",
    "    \n",
    "    for p, t in zip(pred_labels, true_labels):\n",
    "        if p not in cluster_counts:\n",
    "            cluster_counts[p] = []\n",
    "        cluster_counts[p].append(t)\n",
    "    \n",
    "    purity_sum, gini_sum = 0.0, 0.0\n",
    "    for cluster, items in cluster_counts.items():\n",
    "        count = len(items)\n",
    "        label_counts = Counter(items)\n",
    "        max_count = max(label_counts.values())\n",
    "        purity_sum += max_count\n",
    "        \n",
    "        # Gini index for the cluster\n",
    "        gini = 1.0 - sum((c / count) ** 2 for c in label_counts.values())\n",
    "        gini_sum += gini * count\n",
    "    \n",
    "    purity = purity_sum / total\n",
    "    gini_index = gini_sum / total\n",
    "    return purity, gini_index\n",
    "\n",
    "def kman_clustering_batch(data, train_labels, k, batch_size, max_iter=100, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    n_samples, n_features = data.shape\n",
    "    \n",
    "    indices = np.random.choice(n_samples, k, replace=False)\n",
    "    centroids = data[indices].toarray().astype(float)  # Convert to dense\n",
    "\n",
    "    # Initialize cumulative sum and count for each centroid\n",
    "    centroid_sums = np.zeros((k, n_features))\n",
    "    centroid_counts = np.zeros(k, dtype=int)\n",
    "    \n",
    "    for iteration in tqdm(range(max_iter), desc=\"Clustering Progress\"):\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_data = data[start:end].toarray()  # Convert to dense\n",
    "\n",
    "            if batch_data.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute distances between batch_data and centroids\n",
    "            distances = np.linalg.norm(batch_data[:, np.newaxis] - centroids, axis=2)\n",
    "            labels_batch = np.argmin(distances, axis=1)  # shape: (batch_size,)\n",
    "            \n",
    "            # Update centroid sums and counts\n",
    "            for i in range(k):\n",
    "                points_in_cluster = batch_data[labels_batch == i]\n",
    "                n_points = points_in_cluster.shape[0]\n",
    "                if n_points > 0:\n",
    "                    centroid_sums[i] += points_in_cluster.sum(axis=0)\n",
    "                    centroid_counts[i] += n_points\n",
    "        \n",
    "        # Update centroids\n",
    "        for i in range(k):\n",
    "            if centroid_counts[i] > 0:\n",
    "                centroids[i] = centroid_sums[i] / centroid_counts[i]\n",
    "            else:\n",
    "                # Reinitialize centroid to a random data point to handle empty cluster\n",
    "                random_idx = np.random.choice(n_samples)\n",
    "                centroids[i] = data[random_idx].toarray().astype(float)\n",
    "                print(f\"Reinitialized centroid {i} to a random data point.\")\n",
    "        \n",
    "        # Reset sums and counts for next iteration\n",
    "        centroid_sums[:] = 0\n",
    "        centroid_counts[:] = 0\n",
    "    \n",
    "    \n",
    "    labels_all = np.empty(n_samples, dtype=int)\n",
    "    for start in tqdm(range(0, n_samples, batch_size), desc=\"Final Labeling\"):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_data = data[start:end].toarray() \n",
    "        distances = np.linalg.norm(batch_data[:, np.newaxis] - centroids, axis=2)\n",
    "        labels_batch = np.argmin(distances, axis=1)\n",
    "        labels_all[start:end] = labels_batch\n",
    "    \n",
    "    return labels_all, centroids, train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering Progress: 100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n",
      "Final Labeling: 100%|██████████| 19/19 [00:04<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective=18181.92, Purity=0.3219, Gini=0.7652\n",
      "\n",
      " K=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering Progress: 100%|██████████| 10/10 [03:54<00:00, 23.41s/it]\n",
      "Final Labeling: 100%|██████████| 19/19 [00:22<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective=17988.05, Purity=0.3905, Gini=0.7084\n",
      "\n",
      " K=40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering Progress: 100%|██████████| 10/10 [10:56<00:00, 65.65s/it]\n",
      "Final Labeling: 100%|██████████| 19/19 [01:26<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective=17727.22, Purity=0.4483, Gini=0.6511\n",
      "Original Labels:  0                misc.forsale\n",
      "1       talk.politics.mideast\n",
      "2    comp.sys.ibm.pc.hardware\n",
      "3          talk.politics.misc\n",
      "4          talk.politics.misc\n",
      "5             rec.motorcycles\n",
      "6                     sci.med\n",
      "7                   sci.crypt\n",
      "8          talk.politics.guns\n",
      "9              comp.windows.x\n",
      "Name: label, dtype: object\n",
      "Unshuffled Labels:  0                misc.forsale\n",
      "1       talk.politics.mideast\n",
      "2    comp.sys.ibm.pc.hardware\n",
      "3          talk.politics.misc\n",
      "4          talk.politics.misc\n",
      "5             rec.motorcycles\n",
      "6                     sci.med\n",
      "7                   sci.crypt\n",
      "8          talk.politics.guns\n",
      "9              comp.windows.x\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for d, lbl in [(data_20_vecotorize, train_labels)]:\n",
    "    for k in [10, 20, 40]:\n",
    "        print(f\"\\n K={k}\")\n",
    "        pred_labels, centroids, train_labels_fin = kman_clustering_batch(\n",
    "            d, lbl, k, batch_size=1000, max_iter=10, random_state=42\n",
    "        )\n",
    "        # Compute metrics\n",
    "        d_dense = d.toarray()\n",
    "        obj = compute_kmeans_objective(d_dense, pred_labels, centroids)\n",
    "        purity, gini = compute_purity_gini(pred_labels, train_labels_fin)\n",
    "        print(f\"Objective={obj:.2f}, Purity={purity:.4f}, Gini={gini:.4f}\")\n",
    "\n",
    "print(\"Original Labels: \", train_labels[:10])  \n",
    "print(\"Unshuffled Labels: \", train_labels_fin[:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purity Increase when K increase, Gini impurity decrease when K increase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
