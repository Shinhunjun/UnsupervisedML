{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO TO Joyanthan Nanduri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
      " 0.         0.00392157 0.01568627 0.         0.         0.\n",
      " 0.         0.00392157 0.00392157 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01176471 0.\n",
      " 0.14117647 0.53333333 0.49803922 0.24313725 0.21176471 0.\n",
      " 0.         0.         0.00392157 0.01176471 0.01568627 0.\n",
      " 0.         0.01176471 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.02352941 0.         0.4        0.8\n",
      " 0.69019608 0.5254902  0.56470588 0.48235294 0.09019608 0.\n",
      " 0.         0.         0.         0.04705882 0.03921569 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
      " 0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
      " 0.30196078 0.50980392 0.28235294 0.05882353 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00392157 0.         0.27058824\n",
      " 0.81176471 0.8745098  0.85490196 0.84705882 0.84705882 0.63921569\n",
      " 0.49803922 0.4745098  0.47843137 0.57254902 0.55294118 0.34509804\n",
      " 0.6745098  0.25882353 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.00392157 0.00392157 0.         0.78431373 0.90980392 0.90980392\n",
      " 0.91372549 0.89803922 0.8745098  0.8745098  0.84313725 0.83529412\n",
      " 0.64313725 0.49803922 0.48235294 0.76862745 0.89803922 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
      " 0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
      " 0.8745098  0.96078431 0.67843137 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.75686275\n",
      " 0.89411765 0.85490196 0.83529412 0.77647059 0.70588235 0.83137255\n",
      " 0.82352941 0.82745098 0.83529412 0.8745098  0.8627451  0.95294118\n",
      " 0.79215686 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.01176471 0.         0.04705882 0.85882353 0.8627451  0.83137255\n",
      " 0.85490196 0.75294118 0.6627451  0.89019608 0.81568627 0.85490196\n",
      " 0.87843137 0.83137255 0.88627451 0.77254902 0.81960784 0.20392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.02352941 0.\n",
      " 0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
      " 0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
      " 0.96078431 0.46666667 0.65490196 0.21960784 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01568627 0.         0.         0.21568627 0.9254902\n",
      " 0.89411765 0.90196078 0.89411765 0.94117647 0.90980392 0.83529412\n",
      " 0.85490196 0.8745098  0.91764706 0.85098039 0.85098039 0.81960784\n",
      " 0.36078431 0.         0.         0.         0.00392157 0.01568627\n",
      " 0.02352941 0.02745098 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.92941176 0.88627451 0.85098039 0.8745098\n",
      " 0.87058824 0.85882353 0.87058824 0.86666667 0.84705882 0.8745098\n",
      " 0.89803922 0.84313725 0.85490196 1.         0.30196078 0.\n",
      " 0.         0.01176471 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.24313725 0.56862745 0.8\n",
      " 0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
      " 0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
      " 0.87843137 0.95686275 0.62352941 0.         0.         0.\n",
      " 0.         0.         0.07058824 0.17254902 0.32156863 0.41960784\n",
      " 0.74117647 0.89411765 0.8627451  0.87058824 0.85098039 0.88627451\n",
      " 0.78431373 0.80392157 0.82745098 0.90196078 0.87843137 0.91764706\n",
      " 0.69019608 0.7372549  0.98039216 0.97254902 0.91372549 0.93333333\n",
      " 0.84313725 0.         0.         0.22352941 0.73333333 0.81568627\n",
      " 0.87843137 0.86666667 0.87843137 0.81568627 0.8        0.83921569\n",
      " 0.81568627 0.81960784 0.78431373 0.62352941 0.96078431 0.75686275\n",
      " 0.80784314 0.8745098  1.         1.         0.86666667 0.91764706\n",
      " 0.86666667 0.82745098 0.8627451  0.90980392 0.96470588 0.\n",
      " 0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
      " 0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
      " 0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
      " 0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
      " 0.87058824 0.89411765 0.88235294 0.         0.38431373 0.91372549\n",
      " 0.77647059 0.82352941 0.87058824 0.89803922 0.89803922 0.91764706\n",
      " 0.97647059 0.8627451  0.76078431 0.84313725 0.85098039 0.94509804\n",
      " 0.25490196 0.28627451 0.41568627 0.45882353 0.65882353 0.85882353\n",
      " 0.86666667 0.84313725 0.85098039 0.8745098  0.8745098  0.87843137\n",
      " 0.89803922 0.11372549 0.29411765 0.8        0.83137255 0.8\n",
      " 0.75686275 0.80392157 0.82745098 0.88235294 0.84705882 0.7254902\n",
      " 0.77254902 0.80784314 0.77647059 0.83529412 0.94117647 0.76470588\n",
      " 0.89019608 0.96078431 0.9372549  0.8745098  0.85490196 0.83137255\n",
      " 0.81960784 0.87058824 0.8627451  0.86666667 0.90196078 0.2627451\n",
      " 0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
      " 0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
      " 0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
      " 0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
      " 0.70980392 0.80392157 0.80784314 0.45098039 0.         0.47843137\n",
      " 0.85882353 0.75686275 0.70196078 0.67058824 0.71764706 0.76862745\n",
      " 0.8        0.82352941 0.83529412 0.81176471 0.82745098 0.82352941\n",
      " 0.78431373 0.76862745 0.76078431 0.74901961 0.76470588 0.74901961\n",
      " 0.77647059 0.75294118 0.69019608 0.61176471 0.65490196 0.69411765\n",
      " 0.82352941 0.36078431 0.         0.         0.29019608 0.74117647\n",
      " 0.83137255 0.74901961 0.68627451 0.6745098  0.68627451 0.70980392\n",
      " 0.7254902  0.7372549  0.74117647 0.7372549  0.75686275 0.77647059\n",
      " 0.8        0.81960784 0.82352941 0.82352941 0.82745098 0.7372549\n",
      " 0.7372549  0.76078431 0.75294118 0.84705882 0.66666667 0.\n",
      " 0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
      " 0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
      " 0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
      " 0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
      " 0.38823529 0.22745098 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.15686275\n",
      " 0.23921569 0.17254902 0.28235294 0.16078431 0.1372549  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Fashion MNIST data downloaded and normalized.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the images to a range of 0 to 1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "train_images = np.array(train_images.reshape((train_images.shape[0], 28 * 28)))\n",
    "test_images = np.array(test_images.reshape((test_images.shape[0], 28 * 28)))\n",
    "\n",
    "combined_images = np.concatenate((train_images, test_images), axis=0)\n",
    "combined_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "\n",
    "print(combined_images.shape)\n",
    "print(train_images[0])\n",
    "# combined_labels = np.array(pd.concat([train_labels, test_labels], axis=0))\n",
    "print(\"Fashion MNIST data downloaded and normalized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(combined_images.shape)\n",
    "print(combined_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (70000, 784)\n"
     ]
    }
   ],
   "source": [
    "def compute_mean_std(data):\n",
    "\n",
    "    mean = data.mean(axis=0)  \n",
    "    std = data.std(axis=0)\n",
    "    \n",
    "    std[std ==0] = 1\n",
    "    return mean, std\n",
    "\n",
    "def preprocess_mnist(data, mean, std):\n",
    "    \n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "mean, std = compute_mean_std(train_images) \n",
    "combinded_normalized = preprocess_mnist(combined_images, mean, std)\n",
    "\n",
    "\n",
    "\n",
    "print(\"data shape:\", combinded_normalized.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np  \n",
    "\n",
    "def kman_clustering_batch(data, k, batch_size, max_iter=100, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    # Randomly initialize centroids\n",
    "    indices = np.random.choice(len(data), k, replace=False)\n",
    "    centroids = data[indices]\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        for start in range(0, len(data), batch_size):\n",
    "            end = min(start + batch_size, len(data))\n",
    "            batch_data = data[start:end]\n",
    "\n",
    "            distances = np.sqrt(((batch_data - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "            labels_batch = np.argmin(distances, axis=0)\n",
    "\n",
    "            new_centroids = np.array([batch_data[labels_batch == i].mean(axis=0)\n",
    "                                      for i in range(k)])\n",
    "            \n",
    "            centroids = np.where(np.isnan(new_centroids), centroids, new_centroids)\n",
    "\n",
    "    \n",
    "    distances_all = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "    labels_all = np.argmin(distances_all, axis=0)\n",
    "\n",
    "    return labels_all, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result dataset, K=5, Objective=36456772.22, Purity=0.3789, Gini=0.7098\n",
      "Result dataset, K=10, Objective=30685119.08, Purity=0.5527, Gini=0.5704\n",
      "Result dataset, K=20, Objective=26516970.57, Purity=0.6581, Gini=0.4452\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_kmeans_objective(data, labels, centroids):\n",
    "    dist = np.linalg.norm(data - centroids[labels], axis=1)\n",
    "    return np.sum(dist**2)\n",
    "\n",
    "def compute_purity_gini(pred_labels, true_labels):\n",
    "    from collections import Counter\n",
    "    total = len(true_labels)\n",
    "    cluster_counts = {}\n",
    "    for p, t in zip(pred_labels, true_labels):\n",
    "        if p not in cluster_counts:\n",
    "            cluster_counts[p] = []\n",
    "        cluster_counts[p].append(t)\n",
    "\n",
    "    purity_sum, gini_sum = 0.0, 0.0\n",
    "    for cluster, items in cluster_counts.items():\n",
    "        count = len(items)\n",
    "        label_counts = Counter(items)\n",
    "        max_count = max(label_counts.values())\n",
    "        purity_sum += max_count\n",
    "        # Gini for this cluster\n",
    "        gini = 1.0 - sum((c / count)**2 for c in label_counts.values())\n",
    "        gini_sum += gini * count\n",
    "    purity = purity_sum / total\n",
    "    gini_index = gini_sum / total\n",
    "    return purity, gini_index\n",
    "\n",
    "\n",
    "for name, d, lbl in [(\"Result\", combinded_normalized, combined_labels)]:\n",
    "    \n",
    "    for k in [5, 10, 20]:\n",
    "        pred_labels, centroids = kman_clustering_batch(d, k, batch_size=10000, max_iter=100, random_state=42)\n",
    "        \n",
    "        pred_labels = np.clip(pred_labels, 0, len(centroids) - 1)\n",
    "        obj = compute_kmeans_objective(d, pred_labels, centroids)\n",
    "        purity, gini = compute_purity_gini(pred_labels, lbl)\n",
    "        print(f\"{name} dataset, K={k}, Objective={obj:.2f}, Purity={purity:.4f}, Gini={gini:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
