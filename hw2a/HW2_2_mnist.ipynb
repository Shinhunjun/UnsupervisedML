{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO TO Joyanthan Nanduri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Mnist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hunjunsin/.pyenv/versions/3.11.6/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 60000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x1569388D0>, 'label': 5}\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x144CC0F10>, 'label': 7}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#load data (image)\n",
    "mnist = load_dataset(\"mnist\")\n",
    "\n",
    "print(mnist)\n",
    "print(mnist['train'][0])\n",
    "print(mnist['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "combined_data = concatenate_datasets([mnist[\"train\"], mnist[\"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parsing_data(data):\n",
    "    \n",
    "    images = np.array([np.array(row['image'], dtype=np.float32).flatten() for row in data])\n",
    "    labels = np.array([row['label'] for row in data], dtype=np.int32)\n",
    "    return images, labels\n",
    "#flatten image into 764 dim numpy array\n",
    "\n",
    "mnist_data, mnist_labels = parsing_data(combined_data)\n",
    "# train_data, train_labels = parsing_data(mnist_split['train'])\n",
    "# val_data, val_labels=parsing_data(mnist_split['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Validation, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (70000, 784)\n"
     ]
    }
   ],
   "source": [
    "def compute_mean_std(data):\n",
    "\n",
    "    mean = data.mean(axis=0)  \n",
    "    std = data.std(axis=0)\n",
    "    \n",
    "    std[std ==0] = 1\n",
    "    return mean, std\n",
    "\n",
    "def preprocess_mnist(data, mean, std):\n",
    "    \n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "mean, std = compute_mean_std(mnist_data) \n",
    "mnist_normalized = preprocess_mnist(mnist_data, mean, std)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training data shape:\", mnist_normalized.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing Mnist (IMAGE -> ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5n/z64_841d7zn4lzhk5r1t4m700000gn/T/ipykernel_63738/1011972786.py:17: RuntimeWarning: Mean of empty slice.\n",
      "  new_centroids = np.array([batch_data[labels_batch == i].mean(axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centroids: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Train dataset, K=5, Objective=45547968.00, Purity=0.3511, Gini=0.7576\n",
      "Train dataset, K=10, Objective=42717920.00, Purity=0.5017, Gini=0.6339\n",
      "Train dataset, K=20, Objective=40148124.00, Purity=0.5962, Gini=0.5380\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def kman_clustering_batch(data, k, batch_size, max_iter=100, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    # Randomly initialize centroids\n",
    "    indices = np.random.choice(len(data), k, replace=False)\n",
    "    centroids = data[indices] \n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        for start in range(0, len(data), batch_size):\n",
    "            end = min(start + batch_size, len(data))\n",
    "            batch_data = data[start:end]\n",
    "\n",
    "            distances = np.sqrt(((batch_data - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "            labels_batch = np.argmin(distances, axis=0)\n",
    "\n",
    "            new_centroids = np.array([batch_data[labels_batch == i].mean(axis=0)\n",
    "                                      for i in range(k)])\n",
    "            \n",
    "            centroids = np.where(np.isnan(new_centroids), centroids, new_centroids)\n",
    "\n",
    "    \n",
    "    distances_all = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "    labels_all = np.argmin(distances_all, axis=0)\n",
    "\n",
    "    return labels_all, centroids\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "labels, centroids = kman_clustering_batch(mnist_normalized, k=10, batch_size=batch_size)\n",
    "print(\"Cluster centroids:\", centroids)\n",
    "\n",
    "def compute_kmeans_objective(data, labels, centroids):\n",
    "    dist = np.linalg.norm(data - centroids[labels], axis=1)\n",
    "    return np.sum(dist**2)\n",
    "\n",
    "def compute_purity_gini(pred_labels, true_labels):\n",
    "    from collections import Counter\n",
    "    total = len(true_labels)\n",
    "    cluster_counts = {}\n",
    "    for p, t in zip(pred_labels, true_labels):\n",
    "        if p not in cluster_counts:\n",
    "            cluster_counts[p] = []\n",
    "        cluster_counts[p].append(t)\n",
    "\n",
    "    purity_sum, gini_sum = 0.0, 0.0\n",
    "    for cluster, items in cluster_counts.items():\n",
    "        count = len(items)\n",
    "        label_counts = Counter(items)\n",
    "        max_count = max(label_counts.values())\n",
    "        purity_sum += max_count\n",
    "        # Gini for this cluster\n",
    "        gini = 1.0 - sum((c / count)**2 for c in label_counts.values())\n",
    "        gini_sum += gini * count\n",
    "    purity = purity_sum / total\n",
    "    gini_index = gini_sum / total\n",
    "    return purity, gini_index\n",
    "\n",
    "# Evaluate for train, val data with k values 5, 10, 20\n",
    "for  d, lbl in [( mnist_normalized, mnist_labels)]:\n",
    "    \n",
    "    for k in [5, 10, 20]:\n",
    "        pred_labels, centroids = kman_clustering_batch(d, k, batch_size=1000, max_iter=100, random_state=42)\n",
    "        # Ensure labels are within the range of centroids\n",
    "        pred_labels = np.clip(pred_labels, 0, len(centroids) - 1)\n",
    "        obj = compute_kmeans_objective(d, pred_labels, centroids)\n",
    "        purity, gini = compute_purity_gini(pred_labels, lbl)\n",
    "        print(f\"K={k}, Objective={obj:.2f}, Purity={purity:.4f}, Gini={gini:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Increae=> Purity Increae, Gini Decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft Kmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def soft_kmeans_clustering(data, k, batch_size, max_iter=100, beta=1.0, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Randomly initialize centroids\n",
    "    indices = np.random.choice(len(data), k, replace=False)\n",
    "    centroids = data[indices]\n",
    "    \n",
    "    for iteration in tqdm(range(max_iter), desc=\"Soft K-Means\"):\n",
    "        for start in range(0, len(data), batch_size):\n",
    "            end = min(start + batch_size, len(data))\n",
    "            batch_data = data[start:end]\n",
    "            \n",
    "            # Compute distances and responsibilities\n",
    "            distances = np.linalg.norm(batch_data[:, np.newaxis] - centroids, axis=2)\n",
    "            responsibilities = np.exp(-beta * distances**2)\n",
    "            responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "\n",
    "            # Update centroids using batch responsibilities\n",
    "            new_centroids = np.array([\n",
    "                (responsibilities[:, k][:, np.newaxis] * batch_data).sum(axis=0) / responsibilities[:, k].sum()\n",
    "                for k in range(k)\n",
    "            ])\n",
    "            \n",
    "            # Handle NaN values (e.g., if a cluster has no points)\n",
    "            centroids = np.where(np.isnan(new_centroids), centroids, new_centroids)\n",
    "    \n",
    "    # Final responsibilities for all data\n",
    "    distances_all = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
    "    responsibilities_all = np.exp(-beta * distances_all**2)\n",
    "    responsibilities_all /= responsibilities_all.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return responsibilities_all, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Soft K-Means with beta=0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Soft K-Means:   0%|          | 0/50 [00:00<?, ?it/s]/var/folders/5n/z64_841d7zn4lzhk5r1t4m700000gn/T/ipykernel_63738/3783686044.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
      "Soft K-Means: 100%|██████████| 50/50 [00:44<00:00,  1.13it/s]\n",
      "/var/folders/5n/z64_841d7zn4lzhk5r1t4m700000gn/T/ipykernel_63738/3783686044.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  responsibilities_all /= responsibilities_all.sum(axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.1, Purity: 0.4391, Gini Index: 0.6785\n",
      "\n",
      "Running Soft K-Means with beta=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Soft K-Means: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 1, Purity: 0.2789, Gini Index: 0.7838\n",
      "\n",
      "Running Soft K-Means with beta=10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Soft K-Means: 100%|██████████| 50/50 [00:42<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 10, Purity: 0.2789, Gini Index: 0.7840\n"
     ]
    }
   ],
   "source": [
    "for beta in [0.1, 1, 10]:\n",
    "    print(f\"\\nRunning Soft K-Means with beta={beta}...\")\n",
    "    responsibilities, centroids = soft_kmeans_clustering(mnist_normalized, k=10, batch_size=1000, max_iter=50, beta=beta)\n",
    "\n",
    "    soft_labels = np.argmax(responsibilities, axis=1)\n",
    "\n",
    "    purity, gini = compute_purity_gini(soft_labels, labels)\n",
    "    print(f\"beta: {beta}, Purity: {purity:.4f}, Gini Index: {gini:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
