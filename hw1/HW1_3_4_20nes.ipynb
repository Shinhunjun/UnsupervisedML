{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def parse_metadata_and_body(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='latin1') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    meta = {}\n",
    "    lines = content.split(\"\\n\")\n",
    "    body_start = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \": \" in line:\n",
    "            key, value = line.split(\": \", 1)\n",
    "            meta[key.strip()] = value.strip()\n",
    "        else:\n",
    "            body_start = i + 1\n",
    "            break\n",
    "\n",
    "    body = \"\\n\".join(lines[body_start:]).strip()\n",
    "    return meta, body\n",
    "\n",
    "def create_csv_from_newsgroups(data_dir, output_csv):\n",
    "    \n",
    "    fieldnames = [\"id\",\"label\", \"from\", \"subject\",  \"lines\", \"organization\", \"body\"]\n",
    "    \n",
    "    with open(output_csv, mode='w', encoding='utf-8', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for category in os.listdir(data_dir):\n",
    "            category_dir = os.path.join(data_dir, category)\n",
    "            if not os.path.isdir(category_dir):\n",
    "                continue\n",
    "            \n",
    "            for file_name in os.listdir(category_dir):\n",
    "                file_path = os.path.join(category_dir, file_name)\n",
    "                metadata, body = parse_metadata_and_body(file_path)\n",
    "                \n",
    "                writer.writerow({\n",
    "                    \"id\": file_name,\n",
    "                    \"label\": category,\n",
    "                    \"from\": metadata.get(\"From\", \"\"),\n",
    "                    \"subject\": metadata.get(\"Subject\", \"\"),\n",
    "                    \"lines\": metadata.get(\"Lines\", \"\"),\n",
    "                    \"organization\": metadata.get(\"Organization\", \"\"),\n",
    "                    \"body\": body\n",
    "                })\n",
    "\n",
    "train_data_dir = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news-bydate/20news-bydate-train\"\n",
    "train_output_csv = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news_train.csv\"\n",
    "\n",
    "create_csv_from_newsgroups(train_data_dir, train_output_csv)\n",
    "\n",
    "test_data_dir = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news-bydate/20news-bydate-test\"\n",
    "test_output_csv = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news_test.csv\"\n",
    "\n",
    "create_csv_from_newsgroups(test_data_dir, test_output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 15076\n",
      "Validation : 1885\n",
      "Test : 1885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_csv = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news_train.csv\"\n",
    "test_csv = \"/Users/hunjunsin/Desktop/Jun/Unsupervised/hw1/20news_test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_data, temp_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train : {len(train_data)}\")\n",
    "print(f\"Validation : {len(val_data)}\")\n",
    "print(f\"Test : {len(test_data)}\")\n",
    "\n",
    "train_data.to_csv(\"/Users/hunjunsin/Desktop/Jun/Unsupervised/20news_train_split.csv\", index=False)\n",
    "val_data.to_csv(\"/Users/hunjunsin/Desktop/Jun/Unsupervised/20news_val_split.csv\", index=False)\n",
    "test_data.to_csv(\"/Users/hunjunsin/Desktop/Jun/Unsupervised/20news_test_split.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>from</th>\n",
       "      <th>subject</th>\n",
       "      <th>lines</th>\n",
       "      <th>organization</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12056</th>\n",
       "      <td>61084</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>tffreeba@indyvax.iupui.edu</td>\n",
       "      <td>Re: PLANETS STILL: IMAGES ORBIT BY ETHER TWIST</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They must be shipping that good Eau Clair acid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10359</th>\n",
       "      <td>38744</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>tmc@spartan.ac.BrockU.CA (Tim Ciceran)</td>\n",
       "      <td>Re: MPEG Location</td>\n",
       "      <td>21</td>\n",
       "      <td>Brock University, St. Catharines Ontario</td>\n",
       "      <td>Alan Jackson (ajackson@cch.coventry.ac.uk) wro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>54035</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>John Michael Santore &lt;jsbh+@andrew.cmu.edu&gt;</td>\n",
       "      <td>Re: Hockey guest spots...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Sophomore, Mathematics, Carnegie Mellon, Pitts...</td>\n",
       "      <td>&gt;Hi guys....\\n&gt;        I'm looking to answe a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>53559</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>johnh@macadam.mpce.mq.edu.au (John Haddy)</td>\n",
       "      <td>Re: what to do with old 256k SIMMs?</td>\n",
       "      <td>14</td>\n",
       "      <td>Macquarie University</td>\n",
       "      <td>In article &lt;120466@netnews.upenn.edu&gt;, jhaines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7341</th>\n",
       "      <td>15842</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>n9045178@henson.cc.wwu.edu (Sean Dean)</td>\n",
       "      <td>Re: Does Rush read his E-mail?</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Western Washington University</td>\n",
       "      <td>rick@ee.uwm.edu (Rick Miller) writes:\\n\\n&gt;rsil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id             label                                         from  \\\n",
       "12056  61084         sci.space                   tffreeba@indyvax.iupui.edu   \n",
       "10359  38744     comp.graphics       tmc@spartan.ac.BrockU.CA (Tim Ciceran)   \n",
       "1266   54035  rec.sport.hockey  John Michael Santore <jsbh+@andrew.cmu.edu>   \n",
       "4602   53559   sci.electronics    johnh@macadam.mpce.mq.edu.au (John Haddy)   \n",
       "7341   15842         sci.crypt       n9045178@henson.cc.wwu.edu (Sean Dean)   \n",
       "\n",
       "                                              subject lines  \\\n",
       "12056  Re: PLANETS STILL: IMAGES ORBIT BY ETHER TWIST     3   \n",
       "10359                               Re: MPEG Location    21   \n",
       "1266                        Re: Hockey guest spots...  28.0   \n",
       "4602              Re: what to do with old 256k SIMMs?    14   \n",
       "7341                   Re: Does Rush read his E-mail?  23.0   \n",
       "\n",
       "                                            organization  \\\n",
       "12056                                                NaN   \n",
       "10359           Brock University, St. Catharines Ontario   \n",
       "1266   Sophomore, Mathematics, Carnegie Mellon, Pitts...   \n",
       "4602                                Macquarie University   \n",
       "7341                       Western Washington University   \n",
       "\n",
       "                                                    body  \n",
       "12056  They must be shipping that good Eau Clair acid...  \n",
       "10359  Alan Jackson (ajackson@cch.coventry.ac.uk) wro...  \n",
       "1266   >Hi guys....\\n>        I'm looking to answe a ...  \n",
       "4602   In article <120466@netnews.upenn.edu>, jhaines...  \n",
       "7341   rick@ee.uwm.edu (Rick Miller) writes:\\n\\n>rsil...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization, Tfidf vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_20 = train_data.dropna(subset = ['body']).reset_index(drop = True)\n",
    "train_documents = train_20['body'].reset_index(drop=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000, \n",
    "    lowercase=True, \n",
    "    stop_words=\"english\", \n",
    "    max_df=0.8, \n",
    "    min_df=5\n",
    ")\n",
    "train_tfidf_matrix = vectorizer.fit_transform(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_20 = val_data.dropna(subset = ['body']).reset_index(drop = True)\n",
    "test_20 = test_data.dropna(subset = ['body']).reset_index(drop = True)\n",
    "\n",
    "val_documents = val_20['body'].reset_index(drop=True)\n",
    "test_documents = test_20['body'].reset_index(drop=True)\n",
    "\n",
    "val_tfidf_matrix = vectorizer.transform(val_documents)\n",
    "test_tfidf_matrix = vectorizer.transform(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15053, 10000)\n",
      "(1879, 10000)\n",
      "(1885, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf_matrix.shape)\n",
    "print(val_tfidf_matrix.shape)\n",
    "print(test_tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Batches: 100%|██████████| 151/151 [07:13<00:00,  2.87s/it]\n",
      "Processing Train Batches: 100%|██████████| 151/151 [06:56<00:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Distance Matrix Shape: (15053, 1879)\n",
      "Train-Test Distance Matrix Shape: (15053, 1885)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def euc_dis(x, y):\n",
    "    x_dense = x.toarray() \n",
    "    y_dense = y.toarray() \n",
    "    return np.sqrt(np.sum((x_dense[:, np.newaxis, :] - y_dense[np.newaxis, :, :])**2, axis=-1))\n",
    "\n",
    "def batch_euclidean_dis(train_data, other_data, batch_size):\n",
    "    n_train, n_features = train_data.shape\n",
    "    n_other = other_data.shape[0]\n",
    "    \n",
    "    distance_matrix = np.zeros((n_train, n_other))\n",
    "    \n",
    "    for i in tqdm(range(0, n_train, batch_size), desc=\"Train Batches\"):\n",
    "        for j in range(0, n_other, batch_size):\n",
    "            batch_train = train_data[i:i + batch_size]\n",
    "            batch_other = other_data[j:j + batch_size]\n",
    "            \n",
    "            distances = euc_dis(batch_train, batch_other)\n",
    "            distance_matrix[i:i + batch_size, j:j + batch_size] = distances\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "train_val_distances = batch_euclidean_dis(train_tfidf_matrix, val_tfidf_matrix, batch_size=100)\n",
    "train_test_distances = batch_euclidean_dis(train_tfidf_matrix, test_tfidf_matrix, batch_size=100)\n",
    "\n",
    "print(\"Train-Validation Distance Matrix Shape:\", train_val_distances.shape)\n",
    "print(\"Train-Test Distance Matrix Shape:\", train_test_distances.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(train_labels, distance_matrix, k):\n",
    "    n_queries = distance_matrix.shape[1]  \n",
    "    predictions = []\n",
    "\n",
    "    for i in tqdm(range(n_queries)):\n",
    "        \n",
    "        k_nearest_indices = np.argsort(distance_matrix[:, i])[:k]\n",
    "        k_nearest_labels = train_labels[k_nearest_indices]\n",
    "        \n",
    "        unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n",
    "        predicted_label = unique_labels[np.argmax(counts)]\n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def find_best_k(train_labels, val_labels, train_val_distances, k_values):\n",
    "    best_k = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for k in k_values:\n",
    "        val_predictions = knn_predict(train_labels, train_val_distances, k)\n",
    "        \n",
    "        accuracy = np.mean(val_predictions == val_labels)\n",
    "        print(f\"k={k}, Validation Accuracy={accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "    \n",
    "    return best_k, best_accuracy\n",
    "\n",
    "def compute_test_accuracy(train_labels, test_labels, train_test_distances, best_k):\n",
    "    test_predictions = knn_predict(train_labels, train_test_distances, best_k)\n",
    "    accuracy = np.mean(test_predictions == test_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, Validation Accuracy=0.5061\n",
      "k=3, Validation Accuracy=0.3566\n",
      "k=5, Validation Accuracy=0.3204\n",
      "k=7, Validation Accuracy=0.3960\n",
      "k=9, Validation Accuracy=0.7664\n",
      "Best k: 9, Best Validation Accuracy: 0.7664\n",
      "Test Accuracy with k=9: 0.7523\n"
     ]
    }
   ],
   "source": [
    "k_values = [1, 3, 5, 7, 9]\n",
    "best_k, best_val_accuracy = find_best_k(train_20['label'], val_20['label'], train_val_distances, k_values)\n",
    "print(f\"Best k: {best_k}, Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "test_accuracy = compute_test_accuracy(train_20['label'], test_20['label'], train_test_distances, best_k)\n",
    "print(f\"Test Accuracy with k={best_k}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_matrix = train_tfidf_matrix.toarray()\n",
    "val_tfidf_matrix = val_tfidf_matrix.toarray()\n",
    "test_tfidf_matrix = test_tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Batches: 100%|██████████| 151/151 [00:09<00:00, 15.34it/s]\n",
      "Processing Train Batches: 100%|██████████| 151/151 [00:09<00:00, 16.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Similarity Matrix Shape: (15053, 1879)\n",
      "Train-Test Similarity Matrix Shape: (15053, 1885)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    \n",
    "    norm_x = np.linalg.norm(x, axis=-1, keepdims=True) \n",
    "    norm_y = np.linalg.norm(y, axis=-1, keepdims=True) \n",
    "\n",
    "    dot_product = np.dot(x, y.T)  \n",
    "\n",
    "    cosine_sim = dot_product / (norm_x * norm_y.T + 1e-8)  \n",
    "    return cosine_sim\n",
    "\n",
    "def batch_cosine_sim(train_data, other_data, batch_size=100):\n",
    "\n",
    "    n_train = train_data.shape[0]\n",
    "    n_other = other_data.shape[0]\n",
    "    \n",
    "    similarity_mat = np.zeros((n_train, n_other))  \n",
    "\n",
    "    for i in tqdm(range(0, n_train, batch_size), desc=\"Train Batches\"):\n",
    "        for j in range(0, n_other, batch_size):\n",
    "            batch_train = train_data[i:i + batch_size]\n",
    "            batch_other = other_data[j:j + batch_size]\n",
    "            \n",
    "            similarities = cosine_sim(batch_train, batch_other)\n",
    "            similarity_mat[i:i + batch_size, j:j + batch_size] = similarities\n",
    "    \n",
    "    return similarity_mat\n",
    "\n",
    "train_val_similarities = batch_cosine_sim(train_tfidf_matrix, val_tfidf_matrix, batch_size=100)\n",
    "train_test_similarities = batch_cosine_sim(train_tfidf_matrix, test_tfidf_matrix, batch_size=100)\n",
    "\n",
    "print(\"Train-Validation Similarity Matrix Shape:\", train_val_similarities.shape)\n",
    "print(\"Train-Test Similarity Matrix Shape:\", train_test_similarities.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsupervised",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
