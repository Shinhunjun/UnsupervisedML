{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download complete  kosarak.dat\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://fimi.uantwerpen.be/data/kosarak.dat\"\n",
    "save_path = \"kosarak.dat\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(save_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "print(f\"download complete  {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n",
      "['1']\n",
      "['4', '5', '6', '7']\n",
      "['1', '8']\n",
      "['9', '10']\n"
     ]
    }
   ],
   "source": [
    "with open(\"kosarak.dat\", \"r\") as f:\n",
    "    transactions = [line.strip().split() for line in f]\n",
    "\n",
    "for transaction in transactions[:5]:\n",
    "    print(transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Write a Python program which takes as its argument5 the path to a text file of data (assumed to be in the itemset format above) and produces as output to the console a sparse ARFF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_kosarak_to_arff(input_file, output_file):\n",
    "    \n",
    "    itemset = set()\n",
    "    transactions = []\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            transaction = list(map(int, line.strip().split()))\n",
    "            transactions.append(transaction)\n",
    "            itemset.update(transaction)\n",
    "\n",
    "    sorted_items = sorted(itemset)\n",
    "    item_index = {item: idx for idx, item in enumerate(sorted_items)}\n",
    "    \n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(\"@relation kosarak\\n\\n\")\n",
    "        \n",
    "        for item in sorted_items:\n",
    "            file.write(f\"@attribute item_{item} {{0,1}}\\n\")\n",
    "        \n",
    "        file.write(\"\\n@data\\n\")\n",
    "\n",
    "        for transaction in transactions:\n",
    "            sparse_representation = []\n",
    "            for item in transaction:\n",
    "                if item in item_index:\n",
    "                    formatted_str = f\"{item_index[item]} 1\"\n",
    "                    sparse_representation.append(formatted_str)\n",
    "            file.write(f\"{{{','.join(sparse_representation)}}}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        sys.exit(1)\n",
    "    \n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2] \n",
    "    process_kosarak_to_arff(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Use your program to convert the kosarak.dat file to a sparse kosarak.arff. About how long did it take to run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arff : It takes 5.21 seconds.\n",
    "txt: It takes 5.07 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Load the resulting file into Weka (as described above; you should have 41,270 attributes and 990, 002 instances). About how long did it take to load this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 13 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Use Weka’s FP-Growth implementation to find rules that have support count of at least 49, 500 and confidence of at least 99% – record your rules (there should be 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== Run information ===\n",
    "\n",
    "Scheme:       weka.associations.FPGrowth -P 1 -I -1 -N 20 -T 0 -C 0.99 -D 0.01 -U 1.0 -M 0.05\n",
    "Relation:     kosarak\n",
    "Instances:    990002\n",
    "Attributes:   41270\n",
    "[list of attributes omitted]\n",
    "\n",
    "\n",
    "FPGrowth found 2 rules (displaying top 2)\n",
    "\n",
    "1. [item_11=1, item_218=1, item_148=1]: 50098 ==> [item_6=1]: 49866   <conf:(1)> lift:(1.64) lev:(0.02) conv:(84.4) \n",
    "2. [item_11=1, item_148=1]: 55759 ==> [item_6=1]: 55230   <conf:(0.99)> lift:(1.63) lev:(0.02) conv:(41.3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Run the algorithm at least 5 times. Then look to the log and record how much time each took. How does the average time compare to the time necessary to convert the dataset and then load into Weka?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tooks 8.2 seconds on average per run, and took 13 seconds to load data in to Weka and 5.21 seconds to convert dat file in to arff file format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo shown to Archit Sachin Bhonsle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsupervised",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
